"""
Water SCARCE Criticality Assessment Script – Vulnerability Dimension
-------------------------------------------------------------

This script calculates the vulnerability dimension of Water SCARCE. Vulnerability 
captures the potential impact that water scarcity or supply disruption may have on
a given region or sector, reflecting the degree to which water users 
are exposed and able to adapt. It inlcudes four categories.

Importance sub-dimension includes:
    - Economic importance of water
    - Domestically required water demand
    - Dependency on external water resources
    
Adaptation sub-dimension includes:
    - Water storage capacity

The calculations follow the Water SCARCE framework (Marinova et al., 2025) with modifications.

Main outputs:
    - Scaled values for each category (0–1)
    - Aggregated water vulnerability score for use in the overall criticality assessment
    - Map with global vulnerability distribution on regional and country level
      
"""

import country_converter as coco
import geopandas as gpd
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

from matplotlib import pyplot as plt

# Initialize the country converter
cc = coco.CountryConverter()

# %% Categories calculation

# ---- WATER VULNERABILITY

# Load the GeoPackage
intersections_regions = gpd.read_file(
    'intersections_regions.gpkg',
    layer='indicators_data'
    )

intersections_regions_extra = gpd.read_file(
    'intersections_regions_extra.gpkg',
    layer='indicators'
    )

# Remove rows where the country column has empty value
intersections_regions_extra = intersections_regions_extra.dropna(
    subset=['country']
    )

# Set a scaler to scale the results between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))

# %%% Importance

# ---- ECONOMIC IMPORTANCE OF WATER

# Sample data
economic_importance_data = intersections_regions_extra[[
    'country',
    'country code',
    'province',
    'main basins',
    'withdrawal total m3 per basin',
    'withdrawal total m3 per region',
    'withdrawal total m3 per country',
    'irrigation withdrawal m3 per basin',
    'irrigation withdrawal m3 per region',
    'irrigation withdrawal m3 per country',
    'irrigation withdrawal groundwater m3 per basin',
    'irrigation withdrawal groundwater m3 per region',
    'irrigation withdrawal groundwater m3 per country',
    'manufacturing withdrawal m3 per basin',
    'manufacturing withdrawal m3 per region',
    'manufacturing withdrawal m3 per country',
    'manufacturing withdrawal groundwater m3 per basin',
    'manufacturing withdrawal groundwater m3 per region',
    'manufacturing withdrawal groundwater m3 per country',
    'argiculture gdp',
    'manufacturing gdp',
    'total gva',
    'agriculture employment',
    'manufacturing employment',
    'total employment',
    'geometry'
    ]]

# Create a GeoDataFrame
economic_importance = gpd.GeoDataFrame(
    economic_importance_data,
    crs='EPSG:6933'
    )

# Total withdrawals per sector (country level)
economic_importance['irrigation withdrawal total'] = (
    economic_importance['irrigation withdrawal m3 per country']
    + economic_importance[
        'irrigation withdrawal groundwater m3 per country'
        ]
    )

economic_importance['manufacturing withdrawal total'] = (
    economic_importance['manufacturing withdrawal m3 per country']
    + economic_importance[
        'manufacturing withdrawal groundwater m3 per country'
        ]
    )

# Medians (country level) for scaling WI
economic_importance['agri_median'] = (
    economic_importance['irrigation withdrawal total'].median()
    )

economic_importance['man_median'] = (
    economic_importance['manufacturing withdrawal total'].median()
    )

economic_importance['agri_gva_median'] = (
    economic_importance['argiculture gdp'].median()
    )

economic_importance['man_gva_median'] = (
    economic_importance['manufacturing gdp'].median()
    )

# Water Intensity WI (capped at 2 as per formula)
wi_agri_raw = (
    (economic_importance['irrigation withdrawal total']
     / economic_importance['argiculture gdp'])
    / (economic_importance['agri_median']
       / economic_importance['agri_gva_median'])
    )

economic_importance['wi_agri'] = wi_agri_raw.where(wi_agri_raw <= 2, 2)

wi_man_raw = (
    (economic_importance['manufacturing withdrawal total']
     / economic_importance['manufacturing gdp'])
    / (economic_importance['man_median']
       / economic_importance['man_gva_median'])
    )

economic_importance['wi_man'] = wi_man_raw.where(wi_man_raw <= 2, 2)

# Final I (NE) calculation per sector
# using square root and WI × 50 as per image
economic_importance['economic_importance_agri'] = (
    np.sqrt(
        (economic_importance['agriculture employment']
         / economic_importance['total employment'])
        * (economic_importance['irrigation withdrawal total']
           / economic_importance['withdrawal total m3 per country'])
        )
    * economic_importance['wi_agri']
    * 50
    )

economic_importance['economic_importance_man'] = (
    np.sqrt(
        (economic_importance['manufacturing employment']
         / economic_importance['total employment'])
        * (economic_importance['manufacturing withdrawal total']
           / economic_importance['withdrawal total m3 per country'])
        )
    * economic_importance['wi_man']
    * 50
    )

# Combine sectors
economic_importance['economic importance'] = (
    economic_importance['economic_importance_agri'] +
    economic_importance['economic_importance_man']
    )

# Normalize between 0 and 1
economic_importance[['economic_importance_agri']] = (
    scaler.fit_transform(economic_importance[['economic_importance_agri']])
    )

economic_importance[['economic_importance_man']] = (
    scaler.fit_transform(economic_importance[['economic_importance_man']])
    )

economic_importance[['economic importance']] = (
    scaler.fit_transform(economic_importance[['economic importance']])
    )

# --- DOMESTICALLY REQUIRED WATER DEMAND

# Sample data
required_demand_data = intersections_regions_extra[[
    'country',
    'country code',
    'province',
    'main basins',
    'population',
    'argiculture gdp',
    'manufacturing gdp',
    'irrigation withdrawal m3 per country',
    'irrigation withdrawal groundwater m3 per country',
    'manufacturing withdrawal m3 per country',
    'manufacturing withdrawal groundwater m3 per country',
    'domestic withdrawal m3 per country',
    'geometry'
    ]].copy()

# Mark rows with zero withdrawals ---
required_demand_data['no_domestic_data'] = (
    required_demand_data['domestic withdrawal m3 per country'] == 0
    )

required_demand_data['no_agriculture_data'] = (
    (required_demand_data['irrigation withdrawal m3 per country'] == 0) &
    (required_demand_data[
        'irrigation withdrawal groundwater m3 per country'
        ] == 0)
    )

required_demand_data['no_manufacturing_data'] = (
    (required_demand_data[
        'manufacturing withdrawal m3 per country'
        ] == 0)
    & (required_demand_data[
        'manufacturing withdrawal groundwater m3 per country'
        ] == 0)
    )

# Create a GeoDataFrame
required_demand = gpd.GeoDataFrame(required_demand_data, crs='EPSG:6933')

# Set 'geometry' as the active geometry column
required_demand = required_demand.set_geometry('geometry')

# For rows with no data, set domestic_withdrawal_cap
# to NaN so it won’t affect calculations
required_demand['domestic_withdrawal_cap'] = np.where(
    required_demand['no_domestic_data'],
    np.nan,
    required_demand['domestic withdrawal m3 per country']
    / required_demand['population']
    )


def u_shaped_scaling(x, center=18, sharpness=0.0005):
    """U-shaped scaling function centered at 18."""
    return 1 - np.exp(-sharpness * (x - center)**2)


required_demand['domestic_scaled'] = (
    u_shaped_scaling(required_demand['domestic_withdrawal_cap'])
    )

# Calculate efficiency values for manufacturing and agriculture
# Avoid division by zero and calculate manufacturing efficiency
required_demand['manufacturing_eff'] = np.where(
    required_demand['no_manufacturing_data'],
    np.nan,
    required_demand_data['manufacturing gdp']
    / (
       required_demand['manufacturing withdrawal m3 per country']
       + required_demand[
           'manufacturing withdrawal groundwater m3 per country'
           ]
       )
    )

# Same for agriculture
required_demand['agriculture_eff'] = np.where(
    required_demand['no_agriculture_data'],
    np.nan,
    required_demand_data['argiculture gdp']
    / (
       required_demand['irrigation withdrawal m3 per country']
       + required_demand[
           'irrigation withdrawal groundwater m3 per country'
           ]
       )
    )

# Calculate efficiency values for manufacturing and agriculture
# Set upper limit at 95th percentile
upper_cap = required_demand['manufacturing_eff'].quantile(0.95)

required_demand['capped_mfg_eff'] = np.where(
    required_demand['manufacturing_eff'] > upper_cap,
    upper_cap,
    required_demand['manufacturing_eff']
    )

# Same for agriculture
upper_cap = required_demand['agriculture_eff'].quantile(0.95)
required_demand['capped_agr_eff'] = np.where(
    required_demand['agriculture_eff'] > upper_cap,
    upper_cap,
    required_demand['agriculture_eff']
)

# Then take the log
required_demand['log_mfg_eff'] = np.log1p(required_demand['capped_mfg_eff'])
required_demand['log_agr_eff'] = np.log1p(required_demand['capped_agr_eff'])

# Scale between 0 and 1
required_demand[['agriculture_eff']] = (
    scaler.fit_transform(required_demand[['log_agr_eff']])
    )

required_demand[['manufacturing_eff']] = (
    scaler.fit_transform(required_demand[['log_mfg_eff']])
    )

required_demand['agriculture_scaled'] = (
    1 - required_demand['agriculture_eff']
    )

required_demand['manufacturing_scaled'] = (
    1 - required_demand['manufacturing_eff']
    )

required_demand['demand'] = (
    required_demand['domestic_scaled']
    + required_demand['manufacturing_scaled']
    + required_demand['agriculture_scaled']
    )

# Scale between 0 and 1
required_demand[['demand scaled']] = (
    scaler.fit_transform(required_demand[['demand']])
    )

# ----  DEPENDENCY ON OUTSIDE WATER RESOURCE

# Data Loading and Preparation ---
dependency_data = intersections_regions_extra[[
    'country',
    'country code',
    'province',
    'main basins',
    'population',
    'Dependency ratio',
    'desalination capacity (m3/year) 10E9',
    'geometry'
]]

dependency = gpd.GeoDataFrame(dependency_data, crs='EPSG:6933')

# Convert desalination capacity to m³/year
dependency['desalination_capacity_m3'] = (
    dependency['desalination capacity (m3/year) 10E9']
    * 1e9
    )

# Per capita desalination capacity
dependency['desal_per_cap'] = (
    dependency['desalination_capacity_m3']
    / dependency['population']
    )

# Fill NaNs with 0 (no dependency or no desalination data)
dependency[['Dependency ratio', 'desal_per_cap']] = (
    dependency[['Dependency ratio', 'desal_per_cap']]
    .fillna(0)
    )

# Normalisation
scaler = MinMaxScaler()
dependency[['dependency_norm', 'desal_norm']] = scaler.fit_transform(
    dependency[['Dependency ratio', 'desal_per_cap']]
    )

# Desalination compensation
alpha = 0.5  # Weight for desalination in reducing dependency
dependency['adjusted_dependency'] = (
    dependency['dependency_norm']
    * (1 - alpha * dependency['desal_norm'])
    )

# Coastal access bonus ---
landlocked_countries = {
    'Afghanistan', 'Andorra', 'Armenia', 'Austria', 'Azerbaijan',
    'Belarus', 'Bhutan', 'Bolivia', 'Botswana', 'Burkina Faso',
    'Burundi', 'Central African Republic', 'Chad', 'Czech Republic',
    'Eswatini', 'Ethiopia', 'Hungary', 'Kazakhstan', 'Kosovo',
    'Kyrgyzstan', 'Laos', 'Lesotho', 'Liechtenstein', 'Luxembourg',
    'Malawi', 'Mali', 'Moldova', 'Mongolia', 'Nepal', 'Macedonia',
    'Niger', 'Paraguay', 'Rwanda', 'San Marino', 'Serbia', 'Slovakia',
    'South Sudan', 'Switzerland', 'Tajikistan', 'Turkmenistan',
    'Uganda', 'Uzbekistan', 'Vatican City', 'Zambia', 'Zimbabwe'
    }

dependency['has_coast'] = ~dependency['country'].isin(landlocked_countries)

# Apply coastal access bonus (5% reduction if coastal)
coastal_bonus = 0.05
dependency['dependency'] = dependency['adjusted_dependency'] * (
    1 - coastal_bonus * dependency['has_coast'].astype(int)
)

# Ensure values are clipped between 0 and 1
dependency['dependency scaled'] = dependency['dependency'].clip(0, 1)

# %%% Adaptation

# ---- WATER SUBSTITUABILITY

# Sample data
substituability_data = intersections_regions_extra[[
    'country',
    'country code',
    'province',
    'main basins',
    'Dam capacity per capita',
    'geometry'
    ]]

# Create a GeoDataFrame
substituability = gpd.GeoDataFrame(substituability_data, crs='EPSG:6933')

# Replace NaNs with 0 if you want a fully filled output (optional)
substituability['Dam capacity per capita'] = (
    substituability['Dam capacity per capita'].fillna(0)
    )

# Set 'geometry' as the active geometry column
substituability = substituability.set_geometry('geometry')

substituability['water_substitutability'] = (
    substituability['Dam capacity per capita']
    )

# Avoid issues with log(0)
substituability['water_substitutability_log'] = (
    np.log1p(substituability['water_substitutability'])
    )

# Apply MinMaxScaler
substituability[['substitutability scaled']] = (
    scaler.fit_transform(substituability[['water_substitutability_log']])
    )

substituability['substitutability scaled'] = (
    1 - substituability['substitutability scaled']
    )

# ---- FINAL RESULT

# Start with the base GeoDataFrame
vulnerability = (
    intersections_regions_extra[
        ['country', 'province', 'main basins', 'geometry']
        ]
    .copy()
    )

# List of all GeoDataFrames to combine
geo_dataframes_vulnerability = [
    economic_importance, required_demand, dependency, substituability
    ]

# Loop through each GeoDataFrame and add columns to 'vulnerability'
for gdf in geo_dataframes_vulnerability:
    # Loop through each column in the current GeoDataFrame
    for column in gdf.columns:
        if column != 'geometry':  # Don't overwrite the geometry column
            vulnerability[column] = gdf[column]

vulnerability = vulnerability[[
    'country',
    'country code',
    'province',
    'main basins',
    'economic importance',
    'demand scaled',
    'dependency',
    'substitutability scaled',
    'geometry'
    ]]

# Calculate overall vulnerability score (average of 3 indicators)
vulnerability['vulnerability score'] = (
    vulnerability[
        ['economic importance',
         'demand scaled',
         'dependency',
         'substitutability scaled']
        ]
    .sum(axis=1, min_count=4)
    )

# Convert to GeoDataFrame to keep spatial properties
vulnerability = gpd.GeoDataFrame(vulnerability, crs='EPSG:6933')

# Replace NaNs with 0 if you want a fully filled output (optional)
# vulnerability = vulnerability.fillna(0)

# Some extra lines
# Apply MinMaxScaler
vulnerability[['vulnerability score scaled']] = (
    scaler.fit_transform(vulnerability[['vulnerability score']])
    )

# vulnerability['vulnerability score'] = (
#     vulnerability['vulnerability score'].apply(lambda x: max(x, 0))
#     )

# Save as CSV file
vulnerability.to_csv('vulnerability_all.csv', index=False)

(
 vulnerability[[
     'country', 'country code',
     'vulnerability score scaled',
     'geometry'
     ]]
 .sort_values(by='country')
 .to_csv('vulnerability_final.csv', index=False)
 )

# %% Plot the results
# ---- Country results

# Reproject to WGS84 for natural Earth plotting
vulnerability = vulnerability.to_crs(epsg=4326)

# Define number of bins and create equal-width bins between 0 and 1
num_bins = 10
bins = np.linspace(0, 1, num_bins + 1)
bin_labels = [f'{i + 1}' for i in range(num_bins)]

# Create a new column classifying scores into bins (NaNs remain NaN)
vulnerability['quantile_class'] = pd.cut(
    vulnerability['vulnerability score scaled'],
    bins=bins,
    labels=bin_labels,
    include_lowest=True
)

# Create the plot
fig, ax = plt.subplots(figsize=(15, 10), dpi=300)

# Plot the map, using equal interval classification
vulnerability.plot(
    column='vulnerability score scaled',
    cmap='viridis',
    scheme='user_defined',
    classification_kwds={'bins': bins[1:-1]},  # exclude 0 and 1 from legend
    legend=False,
    ax=ax,
    edgecolor='none',
    linewidth=0.0,
    missing_kwds={'color': 'lightgrey', 'label': 'No data'}
)

# Create custom circular legend
cmap = plt.cm.get_cmap('viridis', num_bins)
for i in range(num_bins):
    low, high = bins[i], bins[i + 1]
    label = f'{low:.2f} – {high:.2f}'
    ax = ax,
    edgecolor = 'none',
    linewidth = 0.0,
    color = cmap(i / (num_bins - 1))  # normalize to 0–1
    ax.scatter([], [], color=color, label=label, marker='o', s=100)

# Add "No data" legend entry
ax.scatter([], [], color='lightgrey', label='No data', marker='o', s=100)

# Final legend and map styling
ax.legend(title='Vulnerability', loc='lower left', frameon=False)
ax.axis('off')  # Turn off gridlines and axes

# Save output
plt.savefig('country_vulnerability_map.svg', bbox_inches='tight')
plt.savefig('country_vulnerability_map.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()

# %% Plot
# Reproject
vulnerability = vulnerability.to_crs(epsg=4326)  # Natural Earth projection

# Define number of equal bins and create the bins from 0 to 1
num_bins = 10
bins = np.linspace(0, 1, num_bins + 1)

# Classify data using pd.cut instead of qcut
# (qcut = quantiles, cut = equal intervals)
bin_labels = [f'{i + 1}' for i in range(num_bins)]

vulnerability['quantile_class'] = pd.cut(
    vulnerability['vulnerability score scaled'],
    bins=bins,
    labels=bin_labels,
    include_lowest=True
    )

# Create the plot
fig, ax = plt.subplots(figsize=(15, 10), dpi=300)

# Plot the map with the 'supply risk score' column using quantiles
vulnerability.plot(
    # Use the 'supply risk score' column for coloring
    column='vulnerability score scaled',
    cmap='viridis',
    scheme='user_defined',
    # exclude 0 and 1 from classification bins
    classification_kwds={'bins': bins[1:-1]},
    legend=False,  # We'll make our own legend
    ax=ax,
    edgecolor='none',
    linewidth=0.0,
    missing_kwds={'color': 'lightgrey', 'label': 'No data'}
    )

# Create custom legend with circular markers matching colormap
cmap = plt.cm.get_cmap('viridis', num_bins)
for i in range(num_bins):
    low, high = bins[i], bins[i + 1]
    label = f'{low:.2f} - {high:.2f}'
    color = cmap(i / (num_bins - 1))  # map i to color space [0,1]
    ax.scatter([], [], color=color, label=label, marker='o', s=100)

legend_elements = []  # Initialize the list for the legend elements

# Add the "No value" entry with light grey color to the legend
legend_elements.append(
    ax.scatter(
        [],
        [],
        color='lightgrey',
        label='No value',
        marker='o',
        s=100
        )
    )

# Final legend and map styling
ax.legend(title='Vulnerability', loc='lower left', frameon=False)
# ax.axis('off') # Turn off gridlines and axes
ax.grid(False)

# Save output
plt.savefig('country_vulnerability_map.svg', bbox_inches='tight')
plt.savefig('country_vulnerability_map.png', dpi=300, bbox_inches='tight')
plt.show()
